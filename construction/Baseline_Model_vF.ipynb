{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training -> Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated 2018-09-05\n",
    "### test_date == prediction date: predict 6-1, then test_date = 6-1 as input\n",
    "- change: variable reset: test_date 改动 -1天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/forecast/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # Train to prediction module\n",
    "# - Optimized for speed\n",
    "# - Following Feature Preprocessing Module\n",
    "# \n",
    "# ## updated 2018-08-30: \n",
    "# - fix bug\n",
    "# - output training pred_train, true_train\n",
    "# - output in DataFrame format for faster evaluation speed\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 31 13:46:58 2018\n",
    "\n",
    "@author: yuze\n",
    "@updated: Ester & Rui\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('/Users/esterwang/Desktop/Project/Demand Forecasting/demand_forecasting/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from datetime import date, timedelta, datetime, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## User_defined functions\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "def get_formated_index(quantile_list):\n",
    "    return pd.Float64Index(data=sorted([x/100 for x in quantile_list]), name='Quantile')\n",
    "    \n",
    "def reformat(x, quantile_list, pred_len_list):\n",
    "    temp = pd.DataFrame(np.reshape(x.values, [len(quantile_list), len(pred_len_list)]),\n",
    "             index=get_formated_index(quantile_list), columns=sorted(pred_len_list))\n",
    "    return pd.DataFrame(np.sort(temp.values, axis=0), index=temp.index, columns=temp.columns)\n",
    "\n",
    "def get_rolling_quantile(df, quantile_list, window_list, path=\"feature_data\"):\n",
    "    \"\"\"\n",
    "    Get rolling quantile values\n",
    "    :param df: a dataframe of input sales data\n",
    "    :param quantile_list: a list of quantile values\n",
    "    :param window_list: a list of time windows\n",
    "    :param path: directory to store feature data files\n",
    "    :return: save dataframe as csv file to local disk\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), path)):\n",
    "        os.makedirs(path)\n",
    "    for quantile in quantile_list:\n",
    "        for window in window_list:\n",
    "            quantile_key = 'window_{window}_quantile_{quantile}'.format(window=str(window), quantile=str(quantile))\n",
    "            print quantile_key\n",
    "            temp = df.T.rolling(window).quantile(float(quantile)/100).T\n",
    "            temp.to_csv(path+'/'+quantile_key)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_rolling_mean(df, window_list, path=\"feature_data\"):\n",
    "    \"\"\"\n",
    "    Get rolling mean values\n",
    "    :param df: a dataframe of input sales data\n",
    "    :param window_list: a list of time windows\n",
    "    :param feature_data: directory to store feature data files\n",
    "    :return: save dataframe as csv file to local disk\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), path)):\n",
    "        os.makedirs(path)\n",
    "    for window in window_list:\n",
    "        mean_key = 'window_{window}_mean'.format(window=str(window))\n",
    "        print mean_key\n",
    "        temp = df.T.rolling(window).mean().T\n",
    "        temp.to_csv(path+'/'+mean_key)\n",
    "    return\n",
    "\n",
    "\n",
    "def read_feature_data(quantile_list, quantile_window_list, mean_window_list, path=\"feature_data\"):\n",
    "    \"\"\"\n",
    "    read feature_data from path\n",
    "    :param quantile_list: a list of quantile values\n",
    "    :param window_list: a list of window values\n",
    "    :param path: directory of the feature data\n",
    "    :return: two dictionaries of dataframes\n",
    "    \"\"\"\n",
    "    quantile_feature = dict()\n",
    "    mean_feature = dict()\n",
    "    for window in quantile_window_list:\n",
    "        for quantile in quantile_list:\n",
    "            quantile_key = 'window_{window}_quantile_{quantile}'.format(window=str(window), quantile=str(quantile))\n",
    "            print 'Reading ', quantile_key \n",
    "            df = pd.read_csv(path+'/'+quantile_key, index_col=0)\n",
    "            df.columns = pd.to_datetime(df.columns)\n",
    "            quantile_feature[quantile_key] = df\n",
    "            \n",
    "        \n",
    "    for window in mean_window_list:\n",
    "        mean_key = 'window_{window}_mean'.format(window=str(window))\n",
    "        print 'Reading ', mean_key\n",
    "        df = pd.read_csv(path+'/'+mean_key, index_col=0)\n",
    "        df.columns = pd.to_datetime(df.columns)\n",
    "        mean_feature[mean_key] = df\n",
    "           \n",
    "    return quantile_feature, mean_feature\n",
    "\n",
    "\n",
    "def get_quantile_value(quantile, window, quantile_feature, train_date):\n",
    "    \"\"\"\n",
    "\n",
    "    :param quantile:\n",
    "    :param window:\n",
    "    :param quantile_feature:\n",
    "    :param train_date: datetime object\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_date = (train_date-timedelta(1)).strftime(\"%Y-%m-%d 00:00:00\")\n",
    "    quantile_key = 'window_{window}_quantile_{quantile}'.format(window=str(window), quantile=str(quantile))\n",
    "    data = quantile_feature.get(quantile_key, None)\n",
    "    if data is None:\n",
    "        return\n",
    "    return data.loc[:, train_date]\n",
    "\n",
    "\n",
    "def get_mean_value(window, mean_feature, train_date):\n",
    "    \"\"\"\n",
    "\n",
    "    :param window:\n",
    "    :param mean_feature:\n",
    "    :param train_date:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_date = (train_date-timedelta(1)).strftime(\"%Y-%m-%d 00:00:00\")\n",
    "    mean_key = 'window_{window}_mean'.format(window=window)\n",
    "    data = mean_feature.get(mean_key, None)\n",
    "    if data is None:\n",
    "        return\n",
    "    return data.loc[:, train_date]\n",
    "\n",
    "\n",
    "def get_back_timespan(df, dt, minus, period):\n",
    "    date_common = pd.date_range(start=dt-timedelta(days=minus), periods=period) & df.columns\n",
    "    len_common = len(date_common)\n",
    "    date_diff = pd.date_range(start=dt-timedelta(days=(minus+30)), periods=(period-len_common))\n",
    "#     return df[pd.date_range(start=dt-timedelta(days=minus+period-len_common), periods=period)]\n",
    "    return df[date_diff|date_common]\n",
    "\n",
    "\n",
    "def get_forward_timespan(df, dt, period):\n",
    "    date_common = pd.date_range(start=dt, periods=period) & df.columns\n",
    "    len_common = len(date_common)\n",
    "    if period > len_common:\n",
    "        date_diff = pd.date_range(start=dt+timedelta(30), periods=period)[-(period-len_common):]\n",
    "    else: date_diff = pd.to_datetime([])\n",
    "    return df[date_common|date_diff]\n",
    "\n",
    "\n",
    "def get_summary_stats(df, train_date, model):\n",
    "    \"\"\"\n",
    "\n",
    "    :param df:\n",
    "    :param train_date:\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame()\n",
    "    window_list = [[3, 7, 14, 30, 60, 140], [7, 14, 30, 60, 140]] \n",
    "    for window in window_list[0]:\n",
    "        tmp = get_back_timespan(df, train_date, window, window)\n",
    "        X['diff_%s_mean_2' % window] = tmp.diff(axis=1).mean(axis=1)\n",
    "        X['mean_%s_decay_2' % window] = (tmp * np.power(0.9, np.arange(window)[::-1])).sum(axis=1)\n",
    "        X['median_%s_2' % window] = tmp.median(axis=1).values # median is exactly 50th quantile\n",
    "        X['min_%s_2' % window] = tmp.min(axis=1)\n",
    "        X['max_%s_2' % window] = tmp.max(axis=1)\n",
    "        X['std_%s_2' % window] = tmp.std(axis=1)\n",
    "        \n",
    "    for window in window_list[1]:\n",
    "        tmp = get_back_timespan(df, train_date, window, window)\n",
    "        X['has_sales_days_in_last_%s' % window] = (tmp > 0).sum(axis=1)\n",
    "        X['last_has_sales_day_in_last_%s' % window] = window - ((tmp > 0) * np.arange(window)).max(axis=1)\n",
    "        X['first_has_sales_day_in_last_%s' % window] = ((tmp > 0) * np.arange(window, 0, -1)).max(axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_label(df, train_date, quantile, pred_len, model, is_train):\n",
    "    if not is_train: return\n",
    "    tmp = get_forward_timespan(df, train_date, pred_len)\n",
    "    if model =='M2Q':\n",
    "        y = np.sqrt(np.sum(tmp, axis=1))  # Yuze Quantile regression #^(1/2) smooth\n",
    "    else:\n",
    "        y_sum = []\n",
    "        pred_len_tmp = int(np.max([pred_len, 31]))\n",
    "        for i in range(pred_len_tmp):\n",
    "            y_sum.append(np.sum(get_back_timespan(df, train_date, pred_len_tmp // 2 - i, pred_len), axis=1))\n",
    "        y = np.sqrt(np.percentile(y_sum, quantile, axis=0)).transpose()\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_features(quantile, quantile_window_list, mean_window_list, train_date, quantile_feature, mean_feature): # done\n",
    "    \"\"\"\n",
    "\n",
    "    :param quantile_list:\n",
    "    :param window_list:\n",
    "    :param train_date:\n",
    "    :param quantile_feature:\n",
    "    :param mean_feature:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame()\n",
    "    for window in quantile_window_list:\n",
    "        quantile_column = 'window_{window}_quantile_{quantile}'.format(window=window, quantile=quantile) #TODO: change this hard coded thing\n",
    "        quantile_column_name = 'q_{window}_2017'.format(window=window)\n",
    "        X[quantile_column_name] = get_quantile_value(quantile, window, quantile_feature, train_date)\n",
    "        \n",
    "    for window in mean_window_list:\n",
    "        mean_column = 'window_{window}_mean'.format(window=window)\n",
    "        mean_column_name = 'mean_{window}_2017'.format(window=window)\n",
    "        X[mean_column_name] = get_mean_value(window, mean_feature, train_date)\n",
    "    return X\n",
    "\n",
    "\n",
    "def prepare_dataset(df, train_date, quantile, quantile_window_list, mean_window_list, pred_len, model, is_train):\n",
    "    quantile_mean_data = get_features(quantile, quantile_window_list, mean_window_list, train_date, quantile_feature, mean_feature)\n",
    "    summary_stats_data = get_summary_stats(df, train_date, model)\n",
    "    y = get_label(df, train_date, quantile, pred_len, model,is_train)\n",
    "    all_data = [quantile_mean_data, summary_stats_data]\n",
    "    data = pd.concat(all_data, axis=1)\n",
    "    return data, y\n",
    "\n",
    "\n",
    "def get_train_date(test_date, pred_len, is_far, rolling_num, rolling_span, time_back_num):\n",
    "    if is_far: #Yuze hardcode jump parameters 3\n",
    "        train_date = test_date - relativedelta(months=12) - relativedelta(days = rolling_span * time_back_num) # set earliest jump start \n",
    "    else: # ensure NOT TOO NEAR\n",
    "        pred_len_tmp = int(np.max([pred_len,31]))\n",
    "        train_date = test_date - relativedelta(days= 2 * pred_len_tmp + (rolling_num-1) * rolling_span)  ##########\n",
    "    return train_date\n",
    "\n",
    "\n",
    "def prepare_training_data(df, quantile, is_far, pred_len, model, test_date, mean_window_list, quantile_window_list, is_train, time_back_num = 2, rolling_num=5, rolling_span=7):\n",
    "    if is_train:\n",
    "        X = [None]*rolling_num\n",
    "        y = [None]*rolling_num\n",
    "    \n",
    "        train_date = get_train_date(test_date, pred_len, is_far, rolling_num, rolling_span, time_back_num)\n",
    "        for i in range(rolling_num):\n",
    "            delta = timedelta(days=rolling_span * i)  # Yuze jumps around, from earliest to latest\n",
    "            X_tmp, y_tmp = prepare_dataset(df, train_date + delta, quantile, quantile_window_list, mean_window_list, pred_len, model, is_train)\n",
    "            X[i] = X_tmp\n",
    "            y[i] = y_tmp\n",
    "        X = pd.concat(X, axis=0)\n",
    "        y = [i for yy in y for i in yy]\n",
    "        return X, y\n",
    "    \n",
    "    else: # 2018-08-30 bug fixed\n",
    "        X, _ = prepare_dataset(df, test_date , quantile, quantile_window_list, mean_window_list, pred_len, model, is_train)\n",
    "        return X\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def sort_quantile(df):\n",
    "    # Make sure higher prediction at higher quantiles - re-arrange\n",
    "    temp = df.reindex_axis(sorted(df.columns, key=lambda x: (x[0],x[1])), axis=1)\n",
    "    temp_sort = list(map(lambda x: sorted(x),  temp.values))\n",
    "    temp_df = pd.DataFrame(temp_sort, index = df.index, columns = df.columns)\n",
    "    return temp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path2feature = 'Data/feature_data_all'\n",
    "\n",
    "path_to_file = 'Data/df_train_sales_filled.csv' #imputed sales\n",
    "\n",
    "#path_to_file ='sample_filled.csv' #raw\n",
    "\n",
    "df = pd.read_csv(path_to_file,index_col=0)\n",
    "df.columns = pd.to_datetime(df.columns)\n",
    "\n",
    "\n",
    "Model ='M2Q' #Choice of model\n",
    "is_far=0\n",
    "if is_far == 1:\n",
    "    far = 'Far'\n",
    "else:\n",
    "    far= 'Near'\n",
    "\n",
    "#quantile_list = [50,60]\n",
    "quantile_list = [50,60,70,80,90,95]  # changed from q_str_list\n",
    "#pred_len_list=[14]\n",
    "pred_len_list = [91, 31, 14, 7, 3, 1]\n",
    "\n",
    "\n",
    "#q_str_list = ['{:.2f}'.format(i) for i in q_list]\n",
    "days_per_train = 1 # fixed\n",
    "# if Model =='Q2Q':\n",
    "#     is_far = 1 \n",
    "# elif Model =='M2Q':\n",
    "#     is_far = 0 \n",
    "   \n",
    "    \n",
    "quantile_window_list=[7,14,28,56,112]\n",
    "mean_window_list=[3,7,14,28,56,112]\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {'learning_rate': [0.05], 'n_estimators':[200],'num_leaves': [31]               , 'bagging_freq':[1], 'bagging_fraction':[0.8]               , 'feature_fraction':[0.8]}\n",
    "\n",
    "# param_grid = {'learning_rate': [0.05, 0.03, 0.01], 'n_estimators':[200] \\\n",
    "#               ,'num_leaves': [31, 61, 91, 127] \\\n",
    "#               , 'bagging_freq':[1], 'bagging_fraction':[0.8] \\\n",
    "#               , 'feature_fraction':[1, 0.8 ,0.6]}\n",
    "# param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'n_estimators':[100, 150, 200, 250, 300]}\n",
    "\n",
    "# Cross validation\n",
    "cv=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in preprocessed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  window_7_quantile_50\n",
      "Reading  window_7_quantile_60\n",
      "Reading  window_7_quantile_70\n",
      "Reading  window_7_quantile_80\n",
      "Reading  window_7_quantile_90\n",
      "Reading  window_7_quantile_95\n",
      "Reading  window_14_quantile_50\n",
      "Reading  window_14_quantile_60\n",
      "Reading  window_14_quantile_70\n",
      "Reading  window_14_quantile_80\n",
      "Reading  window_14_quantile_90\n",
      "Reading  window_14_quantile_95\n",
      "Reading  window_28_quantile_50\n",
      "Reading  window_28_quantile_60\n",
      "Reading  window_28_quantile_70\n",
      "Reading  window_28_quantile_80\n",
      "Reading  window_28_quantile_90\n",
      "Reading  window_28_quantile_95\n",
      "Reading  window_56_quantile_50\n",
      "Reading  window_56_quantile_60\n",
      "Reading  window_56_quantile_70\n",
      "Reading  window_56_quantile_80\n",
      "Reading  window_56_quantile_90\n",
      "Reading  window_56_quantile_95\n",
      "Reading  window_112_quantile_50\n",
      "Reading  window_112_quantile_60\n",
      "Reading  window_112_quantile_70\n",
      "Reading  window_112_quantile_80\n",
      "Reading  window_112_quantile_90\n",
      "Reading  window_112_quantile_95\n",
      "Reading  window_3_mean\n",
      "Reading  window_7_mean\n",
      "Reading  window_14_mean\n",
      "Reading  window_28_mean\n",
      "Reading  window_56_mean\n",
      "Reading  window_112_mean\n",
      "0:10:30.082403\n"
     ]
    }
   ],
   "source": [
    "tick = datetime.now()\n",
    "quantile_feature, mean_feature = read_feature_data(quantile_list, quantile_window_list, mean_window_list, path2feature)\n",
    "\n",
    "print datetime.now()-tick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on data on and before: 2017-03-31 00:00:00\n",
      "Training  M2Q _ Near _ q50len91\n",
      "Predicting  M2Q _ Near _ q50len91  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q60len91\n",
      "Predicting  M2Q _ Near _ q60len91  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q70len91\n",
      "Predicting  M2Q _ Near _ q70len91  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q80len91\n",
      "Predicting  M2Q _ Near _ q80len91  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q90len91\n",
      "Predicting  M2Q _ Near _ q90len91  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q95len91\n",
      "Predicting  M2Q _ Near _ q95len91  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q50len31\n",
      "Predicting  M2Q _ Near _ q50len31  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q60len31\n",
      "Predicting  M2Q _ Near _ q60len31  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q70len31\n",
      "Predicting  M2Q _ Near _ q70len31  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q80len31\n",
      "Predicting  M2Q _ Near _ q80len31  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q90len31\n",
      "Predicting  M2Q _ Near _ q90len31  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q95len31\n",
      "Predicting  M2Q _ Near _ q95len31  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q50len14\n",
      "Predicting  M2Q _ Near _ q50len14  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q60len14\n",
      "Predicting  M2Q _ Near _ q60len14  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q70len14\n",
      "Predicting  M2Q _ Near _ q70len14  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q80len14\n",
      "Predicting  M2Q _ Near _ q80len14  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q90len14\n",
      "Predicting  M2Q _ Near _ q90len14  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q95len14\n",
      "Predicting  M2Q _ Near _ q95len14  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q50len7\n",
      "Predicting  M2Q _ Near _ q50len7  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q60len7\n",
      "Predicting  M2Q _ Near _ q60len7  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q70len7\n",
      "Predicting  M2Q _ Near _ q70len7  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q80len7\n",
      "Predicting  M2Q _ Near _ q80len7  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q90len7\n",
      "Predicting  M2Q _ Near _ q90len7  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q95len7\n",
      "Predicting  M2Q _ Near _ q95len7  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q50len3\n",
      "Predicting  M2Q _ Near _ q50len3  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q60len3\n",
      "Predicting  M2Q _ Near _ q60len3  for: 2017-04-01 00:00:00\n",
      "Generating training performance.\n",
      "Training  M2Q _ Near _ q70len3\n"
     ]
    }
   ],
   "source": [
    "## Batch run\n",
    "tmp_date_list = list(map(lambda date: date.strftime('%Y-%m-%d'), [date(2017,4,1) + relativedelta(months=i) for i in range(14)])) # 2017-4 ~ 2018-5 \n",
    "tmp_date_list = list(map(lambda date: date.strftime('%Y-%m-%d'), [date(2017,4,1)])) # 2017-4 ~ 2018-5 \n",
    "\n",
    "\n",
    "\n",
    "for i, tmp_date in enumerate(tmp_date_list):\n",
    "    seed = 1988\n",
    "    train_model = {} # Model: dim(q) * dim(pred_len) = 6 * 6\n",
    "    train_pred = {}\n",
    "    train_true = {}\n",
    "    start_model_train_time=datetime.now()\n",
    "\n",
    "    pred_date = datetime.strptime(tmp_date, '%Y-%m-%d')    \n",
    "    test_date = pred_date - timedelta(days=days_per_train)\n",
    "    \n",
    "  \n",
    "    Model_output = 'Model_'+Model+'_'+far+'_'+tmp_date+'.pl'\n",
    "    Train_output = 'Train_'+Model+'_'+far+'_'+tmp_date+'.pl'\n",
    "    Prediction_output = 'Pred_'+Model+'_'+far+'_'+tmp_date+'.pl' \n",
    "    \n",
    "    gb_df_pred_list = pd.DataFrame()\n",
    "    gb_df_train_pred_list = pd.DataFrame() # Training prediction\n",
    "    gb_df_train_true_list = pd.DataFrame() # Training y   \n",
    "\n",
    "    print 'Training model on data on and before:', test_date\n",
    "\n",
    "    for pred_len in pred_len_list:\n",
    "        \n",
    "        df_pred_list = pd.DataFrame()\n",
    "        df_train_pred_list = pd.DataFrame() # Training prediction\n",
    "        df_train_true_list = pd.DataFrame() # Training y\n",
    "\n",
    "        for q in quantile_list:\n",
    "            \n",
    "            temp_name = 'q'+str(q)+'len'+str(pred_len) \n",
    "            print \"Training \",Model,'_',far,'_',temp_name \n",
    "\n",
    "            if Model =='M2Q':\n",
    "#                 estimator = lgb.LGBMRegressor(random_state=seed, objective='quantile', alpha=q*0.01)\n",
    "                estimator = lgb.LGBMRegressor(random_state=seed, objective='quantile', alpha=q*0.01 \\\n",
    "                                             ,learning_rate =0.05, n_estimators=200, num_leaves = 31 \\\n",
    "                                              , bagging_freq=1, num_threads =8, bagging_fraction=0.8, feature_fraction=0.8)\n",
    "#                   estimator = lgb.LGBMRegressor(random_state=seed, objective='quantile', alpha=q*0.01 \\\n",
    "#                                              ,learning_rate =0.05, n_estimators=200, num_leaves = 31)\n",
    "            elif Model =='Q2Q':\n",
    "                estimator = lgb.LGBMRegressor(random_state=seed, alpha=q*0.01\\\n",
    "                                             ,learning_rate =0.05, n_estimators=200, num_leaves = 31 \\\n",
    "                                              , bagging_freq=1, num_threads =8, bagging_fraction=0.8, feature_fraction=0.8)\n",
    "\n",
    "#### Train ###\n",
    "            tic = datetime.now()\n",
    "            X_train, y_train = prepare_training_data(df, q, is_far, pred_len, Model, test_date, mean_window_list, quantile_window_list, is_train=True)\n",
    "            X_train = np.round(X_train,4)\n",
    "            y_train = np.round(y_train,4)\n",
    "            \n",
    "            estimator.fit(X_train, y_train)\n",
    "            train_model[temp_name] = estimator\n",
    "            \n",
    "            print \"Predicting \",Model,'_',far,'_',temp_name,' for:',pred_date\n",
    "            \n",
    "            X_test= prepare_training_data(df, q, is_far, pred_len, Model, pred_date, mean_window_list, quantile_window_list, is_train=False)\n",
    "            X_test = np.round(X_test, 4)\n",
    "            y_test = estimator.predict(X_test)**2 # y_train has been transformed **(1/2)\n",
    "            y_test = np.round(y_test,4)\n",
    "            \n",
    "            df_pred = pd.DataFrame(y_test, index=X_test.index)\n",
    "            y_test = np.array(y_test).transpose()\n",
    "            df_pred = pd.DataFrame(y_test, index=X_test.index, columns=[(q,pred_len)])\n",
    "            df_pred[(q,pred_len)] = df_pred[(q,pred_len)].apply(lambda x: np.max([x,0]))\n",
    "                        \n",
    "            if len(df_pred_list)==0:\n",
    "                df_pred_list = df_pred\n",
    "            else:\n",
    "                df_pred_list = pd.concat([df_pred_list, df_pred], axis=1)\n",
    "            \n",
    "            ## Training Performances\n",
    "            print 'Generating training performance.'\n",
    "            y_train_pred = estimator.predict(X_train)**2\n",
    "            y_train_true = np.array(y_train)**2\n",
    "            \n",
    "            df_train_pred = pd.DataFrame(y_train_pred, index=X_train.index)\n",
    "            df_train_true = pd.DataFrame(y_train_true, index=X_train.index)\n",
    "            \n",
    "            y_train_pred = np.array(y_train_pred).transpose()\n",
    "            y_train_true = np.array(y_train_true).transpose()\n",
    "            \n",
    "            df_train_pred = pd.DataFrame(y_train_pred, index=X_train.index, columns=[(q,pred_len)]).sort_index()\n",
    "            df_train_true = pd.DataFrame(y_train_true, index=X_train.index, columns=[(q,pred_len)]).sort_index()\n",
    "            \n",
    "            df_train_pred[(q,pred_len)] = df_train_pred[(q,pred_len)].apply(lambda x: np.max([x,0]))\n",
    "            df_train_true[(q,pred_len)] = df_train_true[(q,pred_len)].apply(lambda x: np.max([x,0]))\n",
    "                        \n",
    "            if len(df_train_pred_list)==0:\n",
    "                df_train_pred_list = df_train_pred\n",
    "                df_train_true_list = df_train_true\n",
    "            else:\n",
    "                df_train_pred_list = pd.concat([df_train_pred_list, df_train_pred], axis=1)\n",
    "                df_train_true_list = pd.concat([df_train_true_list, df_train_true], axis=1)\n",
    "                        \n",
    "            \n",
    "            del X_test, y_test, X_train, y_train, df_train_pred, df_train_true, df_pred\n",
    "            \n",
    "        # Sorted by quantile        \n",
    "        df_pred_list_sort = sort_quantile(df_pred_list)        \n",
    "        df_train_pred_list_sort = sort_quantile(df_train_pred_list)        \n",
    "        df_train_true_list_sort = sort_quantile(df_train_true_list)\n",
    "                        \n",
    "        if len(gb_df_pred_list)==0:\n",
    "            gb_df_pred_list = df_pred_list_sort \n",
    "            gb_df_train_pred_list = df_train_pred_list_sort\n",
    "            gb_df_train_true_list = df_train_true_list_sort\n",
    "        else:\n",
    "            gb_df_pred_list =pd.concat([gb_df_pred_list, df_pred_list_sort], axis=1)\n",
    "            gb_df_train_pred_list = pd.concat([gb_df_train_pred_list, df_train_pred_list_sort], axis=1)\n",
    "            gb_df_train_true_list = pd.concat([gb_df_train_true_list, df_train_true_list_sort], axis=1)\n",
    "        \n",
    "        del df_pred_list, df_train_pred_list, df_train_true_list\n",
    "        \n",
    "    print 'Total training time is: ', datetime.now()-start_model_train_time \n",
    "\n",
    "# # ## For batch run    \n",
    "    # Save Model\n",
    "    file = open(os.path.join('Experiment/Baseline/Model',Model_output), 'wb')\n",
    "    pickle.dump(train_model, file)\n",
    "    file.close()    \n",
    "    # Save Training\n",
    "    file = open(os.path.join('Experiment/Baseline/Train',Train_output), 'wb')\n",
    "    pickle.dump(gb_df_train_pred_list, file)\n",
    "    pickle.dump(gb_df_train_true_list, file)\n",
    "    file.close()\n",
    "    \n",
    "    # Save Prediction\n",
    "    file = open(os.path.join('Experiment/Baseline/Predict',Prediction_output), 'wb')\n",
    "    pickle.dump(gb_df_pred_list, file)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Data/X_train.pl','rb')\n",
    "X_train_server = pickle.load(file)\n",
    "X_train_round_server = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.equals(X_train_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_round.equals(X_train_round_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Data/pred.pl','rb')\n",
    "df_pred_server = pickle.load(file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.equals(df_pred_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Data/test_bagging_4threads.pl','rb')\n",
    "X_test_server = pickle.load(file)\n",
    "y_test_server = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.equals(X_test_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(y_test,y_test_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10530.975100000094"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test - y_test_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
